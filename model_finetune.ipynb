{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA7oI9rhJJ0G",
        "outputId": "0a7ef4ed-d257-4fda-90c5-9a0f98e2ff0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers torch tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0r_tRUSJJ0I",
        "outputId": "bce4782f-ce23-43d9-cbc6-cd92e6a7d837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaModel, AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZIH5tSa8JJ0I"
      },
      "outputs": [],
      "source": [
        "class SolidityDataset(Dataset):\n",
        "    def __init__(self, zip_path, tokenizer, max_length=256):\n",
        "        self.zip_path = zip_path\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = []\n",
        "\n",
        "        # Open the zip file\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            # List all files in the zip archive\n",
        "            file_list = zip_ref.namelist()\n",
        "\n",
        "            # Iterate through each file in the zip\n",
        "            for file_name in file_list:\n",
        "                # Check if the file is a .txt file\n",
        "                if file_name.endswith('.txt'):\n",
        "                    # Open the file directly from the zip\n",
        "                    with zip_ref.open(file_name) as file:\n",
        "                        try:\n",
        "                            # Load the JSON data from the file\n",
        "                            data = json.load(file)\n",
        "\n",
        "                            # Get the \"code\" field if it exists\n",
        "                            code = data.get(\"Code\")\n",
        "                            if code:\n",
        "                                self.samples.append(code)\n",
        "                            else:\n",
        "                                print(f\"Warning: 'code' field not found in {file_name}\")\n",
        "\n",
        "                        except json.JSONDecodeError:\n",
        "                            print(f\"Error decoding JSON in file: {file_name}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"Unexpected error with file {file_name}: {e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        code = self.samples[idx]\n",
        "        encoded_input = self.tokenizer(\n",
        "            code,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return encoded_input['input_ids'].squeeze(), encoded_input['attention_mask'].squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB9Bpq_2JJ0J",
        "outputId": "e34ea0b1-ecce-4a65-c08c-1cc835912bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "# Load pre-trained GraphCodeBERT\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
        "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CvUfYFiJJ0K",
        "outputId": "d76541cc-fd42-4a08-ed41-fa692dfe355d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: 'code' field not found in processed_repositories/2023-11-panoptic/src/LiquidityChunk/liquidity.txt\n",
            "Warning: 'code' field not found in processed_repositories/2023-11-panoptic/src/SemiFungiblePositionManager/getUniswapV3PoolFromId.txt\n",
            "Warning: 'code' field not found in processed_repositories/2023-11-panoptic/src/SemiFungiblePositionManager/getAccountFeesBase.txt\n",
            "Warning: 'code' field not found in processed_repositories/2023-11-panoptic/src/SemiFungiblePositionManager/getPoolId.txt\n",
            "Warning: 'code' field not found in processed_repositories/2023-04-eigenlayer/src/IETHPOSDeposit/get_deposit_root.txt\n",
            "Warning: 'code' field not found in processed_repositories/2023-04-eigenlayer/src/IETHPOSDeposit/get_deposit_count.txt\n",
            "Warning: 'code' field not found in processed_repositories/2023-04-eigenlayer/src/IETHPOSDeposit/deposit.txt\n",
            "Warning: 'code' field not found in processed_repositories/2024-10-loopfi/src/PRBProxyRegistry/_installPlugin.txt\n",
            "Warning: 'code' field not found in processed_repositories/2024-10-loopfi/src/SwapAction/_revertBytes.txt\n",
            "Warning: 'code' field not found in processed_repositories/2024-03-abracadabra-money/src/DegenBox/_onBeforeDeposit.txt\n",
            "Warning: 'code' field not found in processed_repositories/2023-01-astaria/src/ITransferProxy/tokenTransferFrom.txt\n",
            "Warning: 'code' field not found in processed_repositories/2023-01-astaria/src/IFlashAction/onFlashAction.txt\n",
            "Warning: 'code' field not found in processed_repositories/2023-01-astaria/src/V3SecurityHook/constructor.txt\n",
            "Warning: 'code' field not found in processed_repositories/2023-01-astaria/src/V3SecurityHook/getState.txt\n",
            "Warning: 'code' field not found in processed_repositories/2023-01-astaria/src/IBeacon/getImpl.txt\n",
            "Warning: 'code' field not found in processed_repositories/2023-01-astaria/src/ISecurityHook/getState.txt\n",
            "Warning: 'code' field not found in processed_repositories/2023-01-astaria/src/CollateralLookup/computeId.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|██████████| 2661/2661 [16:38<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 219.7194443781185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2661/2661 [16:42<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Loss: 93.22134813067161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2661/2661 [16:41<00:00,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Loss: 57.92295193197314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def fine_tune_model(model, dataloader, epochs=3, learning_rate=1e-5):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for input_ids, attention_mask in tqdm(dataloader):\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token embedding for each sequence\n",
        "\n",
        "            # Contrastive or embedding learning objective here\n",
        "            # For simplicity, use L2 regularization as a placeholder for your fine-tuning loss\n",
        "            loss = (embeddings ** 2).sum()  # Simplified objective, replace as needed\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader)}\")\n",
        "\n",
        "# Initialize dataset and dataloader\n",
        "dataset = SolidityDataset(zip_path=\"/content/drive/My Drive/Colab Notebooks/processed_repositories.zip\", tokenizer=tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Fine-tune the model\n",
        "fine_tune_model(model, dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIJYTVwoJJ0L",
        "outputId": "8da204af-f081-47bc-97be-eb974594534a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.12693151e-02  1.67147908e-02 -1.79229043e-02  1.22786751e-02\n",
            "   9.40082893e-02 -1.90160610e-02  6.31893426e-02  8.71188641e-02\n",
            "   2.53150798e-03 -1.71079263e-02  7.69901043e-03  4.02079187e-02\n",
            "  -2.27990355e-02 -9.26613621e-03  6.22524992e-02  2.03210041e-02\n",
            "  -2.13840287e-02  6.57762364e-02  5.41016497e-02 -8.65189172e-03\n",
            "  -7.92333297e-03  7.57061541e-02  6.75280541e-02  1.76334176e-02\n",
            "   4.37132604e-02 -2.33456083e-02  1.92256700e-02  9.74326655e-02\n",
            "  -2.94204266e-03  1.70235448e-02  1.61178894e-02  2.98657287e-02\n",
            "   3.30194049e-02  1.04138954e-02  4.88821492e-02  4.52986732e-02\n",
            "   7.20111057e-02 -1.87147558e-02 -1.24845710e-02  3.20849344e-02\n",
            "   1.55683607e-02 -5.65463230e-02  3.14878896e-02  1.18020233e-02\n",
            "   1.95525866e-02  7.00365752e-02  2.37055942e-02  4.39225957e-02\n",
            "   5.40768579e-02  2.56278887e-02  2.94013385e-04  9.36264917e-02\n",
            "   9.31167454e-02 -8.74448568e-03 -6.14797398e-02  2.96781901e-02\n",
            "  -5.61094843e-02  2.00962648e-02  2.97649931e-02  6.56321784e-03\n",
            "   4.65468615e-02  9.38143730e-02  1.07596964e-02 -7.60130063e-02\n",
            "  -2.35064095e-03 -3.83336954e-02  2.58891582e-02 -6.10249955e-03\n",
            "   2.72782240e-03 -4.88751335e-03 -3.46290204e-03 -4.52834480e-02\n",
            "   4.69360575e-02  4.54231352e-02  9.83185880e-03  1.01354912e-01\n",
            "   2.77512409e-02 -1.17231749e-01  6.39153784e-03 -2.36208644e-02\n",
            "   2.58560479e-02 -3.01399790e-02 -3.11209708e-02 -1.60958916e-02\n",
            "   2.66667474e-02  4.86898459e-02 -2.10995134e-02  1.10089220e-02\n",
            "   1.40829263e-02  4.49519902e-02  1.58820059e-02  2.39006504e-02\n",
            "   3.45229451e-03 -2.24439311e-03  4.98363152e-02 -1.02797057e-02\n",
            "   4.33102250e-03 -6.63608760e-02  2.91145891e-02 -3.96853313e-03\n",
            "  -1.11410525e-02 -1.36176934e-02  7.51747191e-02  1.40044302e-01\n",
            "   2.25347858e-02 -7.85534678e-04 -5.52435499e-03  1.69065017e-02\n",
            "   4.49127564e-03 -4.39147139e-03 -3.14508863e-02  6.41929582e-02\n",
            "   2.41270959e-02  7.87719488e-02  1.28543219e-02  4.43973055e-04\n",
            "  -2.80741509e-02  1.11465640e-02  7.00337589e-02  9.72278044e-02\n",
            "   2.95906160e-02  1.46107838e-01  1.56986136e-02 -9.06212721e-03\n",
            "   2.75842957e-02  2.23547202e-02  2.05716770e-03 -8.22220277e-03\n",
            "   8.15670043e-02  1.34797851e-02  8.09615999e-02  3.53391794e-03\n",
            "   1.13234796e-01  8.75027180e-02  1.42716393e-01 -9.70687019e-04\n",
            "   4.86460924e-02  2.34583300e-02  6.12968020e-03  1.41664955e-03\n",
            "   1.69912726e-02  1.13060698e-03  2.42160261e-02  1.39214308e-03\n",
            "   6.68995734e-03  1.09778330e-01  1.58736594e-02 -6.65467791e-03\n",
            "   3.44907679e-02  1.39483036e-02 -2.01318897e-02 -2.17478424e-02\n",
            "  -7.77613930e-03 -1.02368910e-02 -2.16899440e-02  3.34848575e-02\n",
            "   4.42282148e-02  6.06914386e-02  3.59547660e-02  3.42402868e-02\n",
            "   6.99000210e-02  4.58662249e-02  2.12052353e-02  5.24308048e-02\n",
            "   1.82923693e-02 -5.35002770e-03  3.89263891e-02 -1.10904193e-02\n",
            "   4.53603826e-03  1.85705384e-03 -1.73082203e-02  1.72607694e-02\n",
            "  -3.28781158e-02  1.66758671e-02 -9.95515846e-03  2.95681413e-02\n",
            "   9.29910988e-02 -4.56984621e-03 -4.40068506e-02  1.88608728e-02\n",
            "   7.73200183e-04  1.26031050e-02 -4.27593663e-02  1.77432746e-02\n",
            "  -4.23377613e-03  2.16463469e-02 -5.82247116e-02 -5.92944911e-03\n",
            "   4.13980223e-02 -4.58232164e-02  1.80454925e-02 -2.33101919e-02\n",
            "  -1.24653438e-02  7.02666715e-02  1.20066907e-02 -1.53751276e-03\n",
            "  -1.28842127e-02  3.14730406e-03  7.73466304e-02 -2.92502635e-04\n",
            "  -8.63077492e-03 -4.72879363e-03  5.52096069e-02  1.64367221e-02\n",
            "   1.23943128e-01  5.61403930e-02  7.93622434e-02  4.07345369e-02\n",
            "   5.66143729e-02 -1.26376040e-02 -1.24958605e-02  1.13728650e-01\n",
            "  -3.78098863e-04 -1.06407804e-02 -1.20367259e-02 -7.55424751e-03\n",
            "  -6.18728518e-04  1.35947829e-02  2.38258089e-03 -8.64428561e-03\n",
            "   9.27628502e-02 -3.36533003e-02  6.33468628e-02  7.50350505e-02\n",
            "  -1.21947983e-02 -8.49958882e-03  6.47909269e-02 -4.06521037e-02\n",
            "   9.24160983e-03  8.14824775e-02  6.13160953e-02  1.72038507e-02\n",
            "   1.22653600e-02  9.85186268e-03  1.02864141e-02  2.82364693e-02\n",
            "  -1.52343921e-02  1.17062934e-01 -1.07470820e-04  3.60825472e-03\n",
            "  -3.93974446e-02 -2.75165047e-02  4.82796207e-02  1.81984734e-02\n",
            "  -2.01683419e-04 -3.99099290e-03  4.07264475e-03  1.37496116e-02\n",
            "   7.03974962e-02  1.12643512e-02  3.87074379e-03 -2.80413385e-02\n",
            "   6.67923987e-02 -6.98037632e-03 -3.45528573e-02 -8.44721589e-03\n",
            "  -8.55538622e-03  3.66615020e-02  3.30141955e-03  3.49689573e-02\n",
            "   3.55514772e-02 -2.93534920e-02  7.17980927e-03 -6.81568752e-04\n",
            "  -3.87417786e-02 -1.71367545e-02  9.76273566e-02 -5.65113220e-03\n",
            "  -3.88988340e-03 -2.88888998e-02  3.77214886e-02  2.02918034e-02\n",
            "   5.01506068e-02  1.89244505e-02 -5.90774929e-03 -3.87272947e-02\n",
            "   1.40495989e-02 -1.54660153e-03  6.88343495e-03 -1.76929478e-02\n",
            "   3.61637259e-03 -4.29081917e-02  1.83435772e-02  1.02309333e-02\n",
            "  -5.21467952e-03  5.48910489e-03 -4.94848099e-03 -1.85288042e-02\n",
            "   8.29189569e-02  8.85488186e-03  1.13345878e-02 -1.44117838e-02\n",
            "  -7.24664005e-03 -1.96964741e-02 -1.53539404e-02  1.26040895e-02\n",
            "  -4.35441965e-03  1.05179101e-01  1.19426645e-01  1.81645248e-02\n",
            "  -9.25139815e-04  2.38600690e-02 -5.44092059e-02  3.15725692e-02\n",
            "   8.41015950e-03 -8.38547200e-03 -2.17213985e-02  2.10012477e-02\n",
            "   2.61667278e-02  1.33944061e-02  2.66799540e-03 -2.45431699e-02\n",
            "  -4.37798351e-02  7.81922638e-02  4.92544882e-02 -8.08884203e-03\n",
            "  -1.69544469e-03 -4.87543875e-03  4.62827310e-02  1.36303874e-02\n",
            "  -2.64474154e-02 -1.28358696e-03  4.77419095e-03  5.46895973e-02\n",
            "  -8.36069696e-03  3.83990221e-02 -1.02182208e-02 -3.30230743e-02\n",
            "   2.08202982e-03  4.89111952e-02  2.50055566e-02 -5.59682678e-03\n",
            "   6.80433288e-02  1.96276829e-02  5.47515973e-02  1.00404605e-01\n",
            "  -5.37489494e-03  1.22295521e-01  1.36667237e-01  7.46089369e-02\n",
            "   3.77003290e-03  2.76014153e-02  8.57338123e-03  3.56118903e-02\n",
            "   3.55149843e-02  4.00720835e-02  1.62361488e-02  1.37237431e-02\n",
            "   2.84795202e-02  1.18100243e-02  1.26512134e-02  3.96643169e-02\n",
            "  -1.85316813e-03  1.20133376e-02  9.76836756e-02  1.42477863e-02\n",
            "   7.10654631e-03  7.08093345e-02 -3.72026674e-03  5.10869510e-02\n",
            "  -9.79590975e-03 -1.08152144e-02  8.29135440e-03  4.11624871e-02\n",
            "   2.32237391e-02  1.88343152e-02 -2.12344956e-02 -4.57298243e-03\n",
            "   1.17918546e-03  2.90500596e-02  4.40527424e-02  7.24189207e-02\n",
            "   2.20016632e-02 -3.62027213e-02 -2.01478861e-02 -2.60993578e-02\n",
            "   8.35344046e-02  2.20826026e-02  2.01820303e-02  1.85395882e-01\n",
            "   4.14761603e-02  1.21796742e-01 -4.09083813e-02  1.41091481e-01\n",
            "   9.96844564e-03  2.20257118e-02 -6.32410357e-03  4.37699407e-02\n",
            "   2.48305760e-02 -2.16796389e-03  9.81335808e-03 -1.21656843e-02\n",
            "   4.67161424e-02 -5.71037689e-03  6.46725744e-02 -1.59805454e-02\n",
            "  -7.80680822e-03  1.32955862e-02  1.49093745e-02 -2.43136194e-02\n",
            "  -1.52572070e-03 -7.08440784e-03  2.88345013e-02  8.70638192e-02\n",
            "   1.08040228e-01  3.94615764e-03  6.74060138e-04 -3.06386519e-02\n",
            "   4.82622944e-02 -4.64451276e-02  5.45657054e-02 -3.83966905e-03\n",
            "   4.83402461e-02 -1.72155327e-03  1.12762444e-01  4.39920404e-04\n",
            "  -1.28346272e-02  7.28478143e-03  4.28852960e-02  5.56506738e-02\n",
            "  -4.82949056e-03  1.19586717e-02 -1.11142993e-02  3.30665633e-02\n",
            "   3.62141207e-02 -2.33372096e-02  2.98322439e-02 -6.52549788e-02\n",
            "   5.53104095e-03  3.26917060e-02  4.29785671e-03  1.06074978e-02\n",
            "  -2.23479308e-02 -1.70768481e-02 -1.21157728e-02 -1.22366929e-02\n",
            "   5.56202559e-03  2.41539273e-02 -8.09057802e-03 -9.56959464e-03\n",
            "   6.88377842e-02 -3.35189584e-03  1.09547846e-01  1.08985975e-01\n",
            "  -1.10459398e-03  3.18432972e-02  3.08445916e-02  7.81111047e-02\n",
            "   1.92322712e-02 -1.68807134e-02  9.53115989e-03  9.40736905e-02\n",
            "  -1.45976664e-02 -1.28307000e-01 -2.89171748e-03 -7.37930322e-03\n",
            "   9.90157574e-02  2.60293800e-02  8.47257674e-03  3.96226346e-02\n",
            "  -7.90295843e-03  6.01298437e-02 -1.01314392e-02 -3.48346792e-02\n",
            "   8.96642730e-02 -7.53958803e-03  3.78844552e-02 -4.44650929e-03\n",
            "  -1.02424631e-02 -1.68010686e-02 -6.47477340e-03 -2.43234797e-03\n",
            "  -6.92637218e-03 -5.97333827e-04 -6.24424731e-03 -3.20083549e-04\n",
            "   3.89657393e-02  6.88958168e-02 -1.62116736e-02 -2.17125770e-02\n",
            "  -2.58512851e-02  1.97328646e-02 -1.63393226e-02  8.22981298e-02\n",
            "  -4.54651788e-02  4.94172936e-03 -1.21442573e-02 -5.79011906e-03\n",
            "   8.05322230e-02 -2.63841264e-02  2.09939945e-02 -4.98776399e-02\n",
            "  -1.71428954e-03  1.59070656e-01  3.35339713e-03  6.25577420e-02\n",
            "   4.44455370e-02 -2.11618817e-03 -8.98653567e-02  8.71318299e-03\n",
            "  -5.81516465e-03  4.68025543e-02 -8.63333605e-03  2.80808401e-03\n",
            "  -1.10164937e-02  4.09014449e-02 -7.01503530e-02 -4.07031886e-02\n",
            "   2.53881421e-02  4.22455966e-02  3.29747284e-03  7.68810734e-02\n",
            "   6.30147308e-02  4.54569124e-02  1.33039534e-01  1.17156930e-01\n",
            "   9.21887085e-02  1.19432257e-02 -2.16681231e-02  2.12665442e-02\n",
            "  -2.45074239e-02  5.93275651e-02  6.37103766e-02 -1.24022560e-02\n",
            "   4.97509539e-02 -8.73128027e-02 -4.71795537e-03  3.09374765e-03\n",
            "   3.39668337e-03 -4.13853414e-02  2.60971370e-03 -1.09942891e-02\n",
            "  -6.95652608e-03  6.23155013e-02 -3.01499367e-02  1.52156851e-03\n",
            "  -2.22882889e-02 -4.32989337e-02 -3.07964124e-02 -7.63471844e-03\n",
            "  -1.53636318e-02  3.10481563e-02  7.31941909e-02  5.62763726e-03\n",
            "   1.31781980e-01  1.16948702e-03 -1.28634311e-02  3.20246741e-02\n",
            "  -2.03770250e-02  1.33777382e-02  5.09295799e-02  9.13969055e-02\n",
            "   1.44069018e-02  1.80880504e-03  9.65338142e-04  8.20282772e-02\n",
            "   5.90941012e-02 -3.49144544e-03 -1.71444882e-02  1.10208608e-01\n",
            "  -8.90313741e-03  2.65383627e-02  1.61399320e-02  3.16030569e-02\n",
            "  -2.59520020e-03  4.11903253e-03  5.65565862e-02  1.68102849e-02\n",
            "  -4.98800119e-03 -1.08357193e-02 -2.04006452e-02  3.78069468e-02\n",
            "   5.70283532e-02 -1.66497775e-03  3.97708230e-02  2.10313238e-02\n",
            "   9.19419229e-02  8.87736306e-02 -2.52418802e-03  1.67098582e-01\n",
            "  -2.92927250e-02 -2.36172020e-03  2.56856196e-02 -1.12334988e-03\n",
            "  -1.83317624e-03  6.21078871e-02  1.67700648e-01  7.10335746e-02\n",
            "   8.54079127e-02  1.21438978e-02  2.91644614e-02  7.76698291e-02\n",
            "  -1.29412452e-03  1.00333691e-02  2.83344439e-03  8.19841921e-02\n",
            "   1.02341913e-01  4.17474844e-02 -1.64694767e-02  7.28690624e-03\n",
            "   8.30072153e-04 -6.58932899e-04  4.70228828e-02  5.84883317e-02\n",
            "  -1.13916555e-02 -3.61621417e-02  7.48697072e-02  7.95580894e-02\n",
            "   1.51127772e-02 -2.31740694e-03  8.48752186e-02 -2.05981964e-03\n",
            "  -7.01503009e-02  1.26923155e-02 -4.92566032e-03  1.13615796e-01\n",
            "  -1.51042184e-02  7.25653172e-02  5.56567013e-02  4.96557355e-03\n",
            "  -2.09180396e-02  7.87220299e-02 -5.35501493e-03  4.30777343e-03\n",
            "  -2.01031305e-02 -3.96522470e-02  3.11078709e-02  1.70061197e-02\n",
            "   5.69322072e-02  1.05315549e-02  6.60151569e-03  4.83361538e-03\n",
            "   3.15649472e-02  5.69836721e-02  1.06798066e-02  5.30805513e-02\n",
            "   7.85002112e-02 -1.16950097e-02 -2.79551139e-03 -4.90869172e-02\n",
            "   4.02053678e-03  1.18015949e-02 -1.62964070e-03 -2.24485211e-02\n",
            "   5.55665419e-02 -3.94243747e-02  1.31659344e-01 -2.66961078e-03\n",
            "  -3.93240619e-03  1.81372687e-01 -1.79974064e-02  2.77294759e-02\n",
            "   4.68622474e-03  1.67305712e-02  1.67018212e-02  1.53002515e-01\n",
            "  -1.27924688e-03  8.41537043e-02  4.77248058e-02  2.58731432e-02\n",
            "   2.77356966e-03  1.05093243e-02 -3.99782099e-02 -2.88645625e-02\n",
            "  -2.39938013e-02  1.99618284e-02  1.04160365e-02  4.47778665e-02\n",
            "   2.94827446e-02 -4.07501645e-02 -3.27667198e-03 -1.01956036e-02\n",
            "   4.57947515e-02  5.11583500e-02  3.74656804e-02  7.19498843e-02\n",
            "   1.27084367e-03 -1.03039136e-02  6.64271787e-03  8.97653624e-02\n",
            "   4.35030721e-02  2.66979425e-03  3.49890329e-02  5.21505699e-02\n",
            "  -1.32863456e-02  1.34089065e-03  1.64461359e-02  6.38860613e-02\n",
            "   1.40414005e-02  6.30851239e-02  3.59657183e-02  1.30597770e-01\n",
            "   2.45786663e-02 -3.32795200e-03  1.91354565e-02  7.68556958e-03\n",
            "   4.70872819e-02 -1.37138963e-02 -5.05949892e-02  7.96334669e-02\n",
            "  -1.18714264e-02 -1.65247975e-03  1.31718097e-02 -1.00476772e-03\n",
            "  -1.25196964e-01  2.82814726e-02  5.09006195e-02 -8.84887055e-02\n",
            "   2.45871283e-02  4.80472401e-04 -1.43387867e-02  1.45695424e-02\n",
            "  -3.52797657e-02  1.24944956e-04 -5.68805188e-02 -1.69975702e-02\n",
            "   1.55243305e-02  1.62358917e-02 -1.96329746e-02  1.34029379e-02\n",
            "  -1.03147943e-02 -1.16129899e-02  1.26461508e-02  6.74367920e-02\n",
            "  -1.12721045e-02 -2.36465712e-03  8.60399455e-02  3.48286238e-03\n",
            "   2.92813685e-02  1.99439190e-02  1.11336023e-01 -3.05879712e-02\n",
            "   2.36928333e-02  2.30843965e-02  6.02068081e-02  5.48366755e-02\n",
            "   4.35845628e-02 -1.68412570e-02  2.54731271e-02  6.26510307e-02\n",
            "   6.78767450e-04  6.31361902e-02  2.40760632e-02 -1.45317661e-02\n",
            "   4.36671339e-02 -1.55645618e-02  1.42248971e-02 -1.99819449e-02\n",
            "  -2.14370061e-03 -2.43542287e-02  1.77551303e-02  1.59046501e-02\n",
            "  -8.60980246e-03 -1.83980924e-03  4.04046709e-03  5.91880307e-02\n",
            "   2.43320446e-02  2.00720411e-02 -1.63005460e-02  3.90072539e-03\n",
            "  -4.89352457e-03  5.16592115e-02  7.35497102e-03  6.62440360e-02\n",
            "   2.09103003e-02 -2.79761851e-02 -3.55743542e-02  3.28768864e-02]]\n"
          ]
        }
      ],
      "source": [
        "def get_embeddings(model, code_snippet):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        encoded_input = tokenizer(\n",
        "            code_snippet,\n",
        "            max_length=256,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoded_input['input_ids'].to(device)\n",
        "        attention_mask = encoded_input['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        embeddings = outputs.last_hidden_state[:, 0, :]  # Use CLS token embedding\n",
        "        return embeddings.cpu().numpy()\n",
        "\n",
        "# Example usage:\n",
        "code_snippet = \"function add(uint256 a, uint256 b) public pure returns (uint256) { return a + b; }\"\n",
        "embedding = get_embeddings(model, code_snippet)\n",
        "print(embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfzO-a_HJJ0L",
        "outputId": "afe99667-4ae4-4b5f-893d-91fd0dc9ef43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/My Drive/finetuned_model/\n"
          ]
        }
      ],
      "source": [
        "# Define the directory to save the model\n",
        "save_dir = '/content/drive/My Drive/finetuned_model/'\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "print(f\"Model saved to {save_dir}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}